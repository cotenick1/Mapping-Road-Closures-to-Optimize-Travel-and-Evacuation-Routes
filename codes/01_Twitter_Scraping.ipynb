{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Twitter Scraping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to begin the data collection process of the project by collecting posts from the social media app Twitter.  Since Twitter limits the amount of tweets collected to just the past 7 days, we would be limited in our data collection.  Therefore, we chose an alternative route - GetOldTweets3.  GetOldTweets3 was developed by Jefferson Henrique ([GitHub](https://github.com/Jefferson-Henrique/GetOldTweets-python)) as a work-around to the Twitter limit.  It works by mimicing a Twitter search on the desktop site aand scraping the requested tweets.\n",
    "\n",
    "Using GetOldTweets3, we are able to collect the following for our process:\n",
    "- tweets from New Jersey 511 accounts and it's partners\n",
    "- the aforementioned tweets filtered by the below keywords:\n",
    "    - 'clos' (in order to account for words such as \"close\", \"closure\")\n",
    "    - ' ' (in order to pull a wide range of tweets to analyze)\n",
    " \n",
    "After a sufficient amount of tweets is collected, we will export the results to the csv file [scraped_tweets](./datasets/scraped_tweets.csv).  The csv format will allow us to pull the data into a seperate notebook in order to clean the data and prepare it for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "---\n",
    "\n",
    "\n",
    " - [Import Resources](#Import-Resources)\n",
    " - [Scrape Twitter](#Scrape-Twitter)\n",
    " - [Organize and Save](#Organize-and-Save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the necessary libraries to enable Twitter scraping.  Furthermore, we read-in the Twitter account csv file for 511NJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read-in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>twitter_handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>511NJ</td>\n",
       "      <td>511njgsp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>511NJ</td>\n",
       "      <td>511njace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>511NJ</td>\n",
       "      <td>511njtpk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>511NJ</td>\n",
       "      <td>511nj55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>511NJ</td>\n",
       "      <td>511nj42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source twitter_handle\n",
       "0  511NJ       511njgsp\n",
       "1  511NJ       511njace\n",
       "2  511NJ       511njtpk\n",
       "3  511NJ        511nj55\n",
       "4  511NJ        511nj42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/njtwitteraccounts.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scrape Twitter\n",
    "---\n",
    "Search for tweets posted from the above list of twitter accounts.\n",
    "\n",
    "- for our first pull (*training data*), we used the following parameters:\n",
    "    - keywords = ['clos']\n",
    "    - max_tweets = 10000\n",
    "    - .setSince('2019-10-24')\n",
    "    - .setUntil('2019-11-07')\n",
    "    \n",
    "    \n",
    "- for our second pull (*testing data*), we used the following parameters:\n",
    "    - keywords = ['']\n",
    "    - max_tweets = 50000\n",
    "    - .setSince('2019-10-24')\n",
    "    - .setUntil('2019-11-07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WARNING:*** *this process takes about 1 hour to run, given the amount of tweets we are scraping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets scraped: 8015\n"
     ]
    }
   ],
   "source": [
    "users = df['twitter_handle'].tolist()\n",
    "keywords = ['']\n",
    "max_tweets = 50000\n",
    "\n",
    "username = []\n",
    "text = []\n",
    "date = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(users)\\\n",
    "                                               .setSince('2019-10-24')\\\n",
    "                                               .setUntil('2019-11-07')\\\n",
    "                                               .setQuerySearch(keyword)\\\n",
    "                                               .setMaxTweets(max_tweets)\n",
    "    \n",
    "    tweets_collected = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    print(f\"Total tweets scraped: {len(tweets_collected)}\")\n",
    "    \n",
    "    for tweet in tweets_collected:\n",
    "        username.append(tweet.username)\n",
    "        text.append(tweet.text)\n",
    "        date.append(tweet.date)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Organize and Save\n",
    "---\n",
    "We will organize the results of the scrape into a usable dataframe and save it down as a csv for use by other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining lists into dataframe\n",
    "scraped_tweets = pd.DataFrame({'username': username,\n",
    "                                'tweet': text,\n",
    "                                'date_posted': date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>511njbt</td>\n",
       "      <td>Delays on George Washington Bridge westbound f...</td>\n",
       "      <td>2019-11-06 23:59:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>511njbt</td>\n",
       "      <td>Delays on George Washington Bridge westbound f...</td>\n",
       "      <td>2019-11-06 23:58:57+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>511njtpk</td>\n",
       "      <td>Crash on New Jersey Turnpike - Eastern Spur so...</td>\n",
       "      <td>2019-11-06 23:58:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>511nji295</td>\n",
       "      <td>Crash on I-295 southbound South of Exit 29 - U...</td>\n",
       "      <td>2019-11-06 23:56:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>511njace</td>\n",
       "      <td>Construction, bridge painting on Atlantic City...</td>\n",
       "      <td>2019-11-06 23:52:57+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    username                                              tweet  \\\n",
       "0    511njbt  Delays on George Washington Bridge westbound f...   \n",
       "1    511njbt  Delays on George Washington Bridge westbound f...   \n",
       "2   511njtpk  Crash on New Jersey Turnpike - Eastern Spur so...   \n",
       "3  511nji295  Crash on I-295 southbound South of Exit 29 - U...   \n",
       "4   511njace  Construction, bridge painting on Atlantic City...   \n",
       "\n",
       "                date_posted  \n",
       "0 2019-11-06 23:59:56+00:00  \n",
       "1 2019-11-06 23:58:57+00:00  \n",
       "2 2019-11-06 23:58:56+00:00  \n",
       "3 2019-11-06 23:56:56+00:00  \n",
       "4 2019-11-06 23:52:57+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv file\n",
    "scraped_tweets.to_csv('../datasets/scraped_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
